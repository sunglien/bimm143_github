---
title: "Class7: Machine Learning 1"
author: "Sung Lien A16628474"
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering and dimensionality reduction methods.

Let's start by making up some data (where we know there are clear groups) that we can use to test out different clustering methods.


We can use the `rnorm()` function to help us here:
```{r}
hist(rnorm(n=3000, mean=3))
```


Make data with `z` two "clusters"

```{r}
x <- c( rnorm(30, mean=-3),
rnorm(30, mean=+3) )

z <- cbind(x=x,y=rev(x))
head(z)

plot(z)
```


How big is `z`
```{r}
nrow(z)
ncol(z)
```


## K-means clustering

The main function in "base" R for K-means clustering is called `kmeans()`
```{r}
k <- kmeans(z, center = 2)
k
```

```{r}
attributes(k)
```

> Q. How many points lie in each cluster?

```{r}
k$size
```


>Q. What compoenent of our results tells us about the cluster membership (ie. which point likes in which cluster)?

```{r}
k$cluster
```


> Q. Center of each cluster?

```{r}
k$center
```




> Q. Put this result info together and make a litter :base R" plot of our clusterinf result. Also add the cluster center points to this plot.

```{r}
plot(z, col="blue")
```



```{r}
plot(z, col= c("blue", "red"))
```

You can color by number. 
```{r}
plot(z, col=c(1,2))
```


Plot colored by cluster membership:

```{r}
plot(z, col=k$cluster)
points(k$centers, col="blue", pch=15)
```



>Q. Run Kmeans on out input `z` and define 4 clusters making the same result vizualization plot as above (plot of z collored by cluster membership).

```{r}
k4 <- kmeans(z, centers = 4)
plot(z, col=k4$cluster)
```



## Hierarchial Clustering

The main function in base R for this called `hclust()` it will take as input a distance matrix (key point is that you can't just give your "raw" data as input - you have to first calculate a distance matrix from your data).

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=10, col="red")
```


```{r}
plot(hc)
```



Once I inspect the "tree" I can "cut" the tree to yield my groupings or clusters. The function to this is called `cutree()`

```{r}
grps <- cutree(hc, h=10)
```


```{r}
plot (z, col=grps)
```

## Hands on with Principal Component Analysis (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similar and if so how?

### Data import
```{r}
url <-"https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```


> Q. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
dim(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

> Q. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)

```



Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks! There must be a better way...

### PCA(Principal Component Anylysis) to the rescue!

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our inpuot data - i.e the important foods in as columns and the countries as rows.

```{r}
pca <- prcomp( t(x) )
summary(pca)
```


Let's see what is in our PCA result object `pca`

```{r}
attributes(pca)
```


The `pca$x` result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis" (a.k.a. "PCS", "eigenvectors", etc.)


```{r}
head(pca$x)
```

```{r}
plot(pca$x[,1], pca$x[,2], pch =16,
     col=c("orange","red","blue","darkgreen"),
     xlab="PC1", ylab= "PC2")
```



We can look at the so-called PC "loadings" result object to see how the original foods contribute to our new PCs(i.e how the original variables contribute to our new better PC variables).

```{r}
pca$rotation[,1]
```



> Q. Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?


```{r}
plot(pca$rotation[,2], pch =16,
     col=c("orange","red","blue","darkgreen"),
     xlab="PC2", ylab= "PC2")
```

> Q How many genes and samples are in this data set?

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
```{r}
nrow(rna.data)
ncol(rna.data)
```